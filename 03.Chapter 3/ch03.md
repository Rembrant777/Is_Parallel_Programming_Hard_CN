## Chapter 3
## Hardware and its Habits 
## 硬件及其特性

Most people intuitively understand that passing messages between systems is more expensive than performing simple calculations within the confines of a single system. 
大多数人会下意识认为系统间进行消息的传输所带来的系统开销要远高于单节点计算机上执行简单的计算. 


But it is also the case that communicating among threads within the confines of a single shared-memory system can also be quite expensive. 
但是同样在这个问题范畴内, 单个计算节点上多线程间彼此间竟有共享内存系统进行的通信其所带来的资源开销也同样高昂. 

This chapter therefore looks at the cost of synchornization and communication within a shared-memory system. 
所以本章就围绕(单个计算节点上的)在共享内存系统上所执行的同步操作与沟通所带来的系统开销来展开.  

These few pages can do no more than scratch the surface of shared-memory parallel hardware design; 
这一章节中的几页呢, 对于设计硬件中涉及到共享内存并行这个话题简直就是隔靴搔痒. 

readers desiring more detail would do well to start with a recent edition of Hennessy’s and Patterson’s classic text. 
渴望了解更多细节的读者可以去阅读 Hennessy’s 与 Patterson 最新版的经典书籍. 

Quick Quiz 
快速自测

3.1: Why should parallel programmers bother learning low-level properties of the hardware ? 
Wouldn’t it be easier, better, and more elegant to remain at a higher level of abstraction ? 
3.1: 为什么并行开发者不厌其烦的学习硬件底层的知识, 而不将时间投入到更加简单, 高效且优雅的高层抽象设计层级上? 

### 3.1 Overview 
### 3.1 概览

Careless reading of computer-system specification sheets might lead one to believe that CPU performance is a footrace on a clear track, as illustrated in Figure 3.1, where the race always goes to the swiftest. 
只是粗略阅读计算机系统规格表的话, 会很容易让人认为 CPU 的性能犹如在明晰跑道上竞技跑的选手一样, 总是跑得最快的先抢占到资源, 就像图中 3.1 所描绘的那样. 



Although there are a few CPU-bound benchmarks that approach the ideal case shown in Figure 3.1, the typical program more closely resembles an obstacle course than a race track. 
尽管在进行基准(benchmarks)测试时, 一些 CPU 密集型的测试用例得到的测试结果贴近图中 3.1 所描述的理想情况, 但更多的常规测试用例得到的指标则更像是一条阻碍赛道, 而不是一条阳关大道上奔跑的情景. 

This is because the internal architecture of CPUs has changed dramatically over the past few decades, courtesy of Moore's Law. These changes are described in the following sections. 

这一方面原因是因为 CPU 的内部架构在过去的十年中谨遵摩尔定律发生了戏剧性的迭代与升级. 这些迭代升级引起的变化会在后续的小节中一一论述. 

### 3.1.1 Pipelined CPUs 
In the 1980s, the typical microprocessor fetched an instruction, decoded it, and executed it, typically taking at least three clock cycles to complete one instruction before even starting the next. 
在 90 年代, 常规的微处理器至少会花费 3 个时钟周期来完成: 获取指令, 解析指令, 运行指令这些操作, 并且在完成之前是无法处理后续指令的.  

In contrast, the CPU of the late 1990s and of the 2000s execute many instructions simultaneously, using pipelines; superscalar techniques; out-of-order instruction and data handling; speculative execution, and more [HP 17, HP 11] in order to optimize the flow of instructions and data through the CPU. 
与之相反, 1990 末 至 2000 年初所生产的 CPU 能够借助于流水线设计独自运行多条指令; 超标量技术; 乱序指令与数据处理; 预测执行, 甚至是[HP 17, HP 11] 这些技术的引用目的只有一个: 对经由 CPU 处理的指令序列与数据进行优化.

Some cores have more than one hardware thread, which is variously called simultaneous multithreading (SMT) or hyperthreading (HT)[Fend73], each of which appears as an independent CPU to software, at least from a functional viewpoint. 
有些内核有多个硬件线程, 这种通常叫做同时多线程(SMT) 或是超线程(HT)[Fend73], 其中的每个线程从软件视角来看都是个独立的 CPU, 至少从软件中函数调用的视角来看是这样的. 

These modern hardware features can greatly improve performance, as illustrated by Figure 3.2.
当代将线程硬件化的这一种特性能极大程度提高性能, 如图 3.2 所示. 

Achieving full performance with a CPU having a long pipeline requires highly predictable control flow through the program. 
在长流水线的程序性能的提升, 需要对程序中的去控制流程有着很高的推断预测的能力. 
若要在具有长流水线的 CPU 实现全面性能的提升, 需要对程序中的控制流有这高度的推断与预测的能力. 

Suitable control flow can be provided by a program that executes primarily in tight loops, for example, arithmetic on large matrices or vectors. 
恰当的控制流主要由循环密集的程序所贡献, 例如，在大型矩阵或向量上进行的算术运算。

The CPU can then correctly predict that the branch at the end of the loop will be taken in almost all cases, allowing the pipeline to be kept full and the CPU to execute at full speed. 
CPU 几乎可以在所有情况下都能够正确预测循环末尾的分支是否将会被执行, 这种预测能够让流水线保持满载状态, 以确保 CPU 能以最大速度运行. 

However, branch prediction is not always so easy. For example, consider a program with many loops, each of another example, consider an old-school object-oriented program with many virtual objects that can reference many different real objects, all with different implementations for frequently invoked member functions, resulting in many calls through pointer. 
但即便如此, 分支预测的实现并非易事. 举例而言, 考虑到一个包含多个循环的程序: 每个循环迭代都包含一小段随机生成数字且执行多次的代码逻辑, 在这种程序上执行分支预测就很复杂. 另一个例子, 考虑到包含众多虚拟对象, 且每个虚拟对象中通过引用关联其他实例对象这样的经典面向对象程序, 其中通过指针发起的频繁成员函数调用时同样会家具分支预测的复杂度. 

In these cases, it is difficult or even impossible for the CPu to predict where the next branch might lead. 
在处理上述例子中, 对于 CPU 而言预测下个分支跳往何处将会是非常复杂且不可能实现的事情. 

Then either the CPU must stall waiting for execution to proceed far enough to be certain where that branch leands, or it must guess and then proceed using speculative execution. Although guessing works extremely well for programs with predictable control flow, for unpredictable branches (such as those in binary search) the guesses will frequently be wrong. 
所以每当执行到这种代码逻辑时, CPU 要不就是暂停执行等到下个分支走向明确时再继续, 要不就是它不得不猜多个分支然后启用预测执行把可能得分支都走一遍. 虽然预测分支执行这种方式在处理偏控制流程这类代码中出奇的有效, 但对于(像是二分搜索) 这种非预测分支这类代码中预测的结果经常会出错. 


A wrong guess can be expensive because the CPU must discard any speculatively executed instructions following the corresponding branch, resulting in a pipeline flush. 
错误的预测带来的开销十分巨大, 这是因为 CPU 不得不将已经完成对应预测分支所执行完成所得到的结果给丢弃掉, 然后清掉该流水线. 

If pipeline flushes appear too frequently, they diastically reduce overall performance, as fancifully depicted in Figure 3.3.
如果  CPU 流水线被频繁的清空, 这将会极大程度上将整体性能给拖垮掉, 图 3.3 描绘的就是这种情况. 

This gets even worse in the increasingly common case of hyperthreading (or SMT, if you prefer), especially on pipelined superscalar out-of-order CPU featuring speculative execution.
在越来越普遍地在支持诸如流水线、超标量和乱序执行功能的 CPU 上启用超线程（或者如果您更习惯于称之为 SMT，即 Simultaneous Multithreading 同时多线程），只会让情况变得更加复杂。

_**hyperthreading:** hyperthreading is a computer professor technology aimed at improving the performance and efficiency of the professor. Simply put, it simulates multiple logical processor cores on a signal physical processor core, allowing the processor to execute multiple threads or tasks simultaneously._

_**SMT:** SMT stands for Simultaneous Multithreading. It is a processor technology similar to Intel's HyperThreading, allowing a single physical processor core to execute multiple threads simultaneously._


In this increasingly common case, all the hardware threads sharing a core also share that core's resources, including registers, cache, execution units, and so on.
在这种日趋增长的使用案例中, 所有硬件线程均共用 1 个核同样也共享该核上的所有资源, 包括注册器, 缓存, 执行单元等等. 
在这种日趋普通的案例中, 所有硬件线程共享一个处理器内核包括这个处理器内核上的寄存器, 缓存和执行单元等等. 

_**hardware threads:** Hardware threads refer to the execution threads supported at the hardware level within a computer pocessor. Each hardware thread is an independent execution unit within the processor, capale of executing instructions autonomously. Hardware threads are often associated with physical processor cores, where a single physical core may contain one or more hardware threads._

The instructions are often decoded into micro-operations, and use of the shared execution units and the hundreds of hardware registers is often coordinated by a micro-operation scheduler. 
这些(计算机)指令通常会被解码成微操作, 并且共享的执行单元以及数百个硬件层级寄存器的使用通常会由微指令调度器协调. 

_**micro-operation:** A micro-operation, also known as a micro-op, refers to a low-level operation performed by a computer processor as part of executing a single instruction from a higher-level programming language or instruction set architecture._

A rough diagram of such a two-threaded core is shown in Figure 3.4, and more accurate (and thus more complex) diagrams are available in text books and scholarly papers. 
对于同时运行 2 个线程的处理器单核的简单示意图在 图 3.4 中, 而更为准确(因此更为复杂)的图示可以在教科书和学术论文中找到. 

Therefore, the execution of one hardware thread can and often is perturbed by the actions of other hardware threads sharing that core. 
也正因如此, 单个硬件线程的执行往往会受到共享同一处理器内核的其他硬件线程的影响.

Even if only one hardware thread is active (for example, in old-school CPU designs where there is only one thread), counterintuitive results are quite common.
即便只有一个硬件线程处于活跃状态(例如, 在包含 1 个线程学校古早 CPU 设计中), 出现出人的意料的结果也是再普遍不过的现象. 


Execution units often have overlapping capabilities, so that a CPU's choice of execution unit can result in pipeline stalls due to contention for that execution unit from later instructions. 

In theory, this contention is avoidable, but in practice clairvoyance. 

In particular, adding an instruction to a tight loop can sometimes actually cause execution to speed up.

Unfortunately, pipeline flushes and shared-resource contention are not the only hazards in the obstacle course that modern CPUs must run.

The next section covers the hazards of referencing memory. 

























